{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565a5d42",
   "metadata": {},
   "source": [
    "\n",
    "# Natural Language Processing 📝 — A Complete Guide (CodSoft Task 4: Spam SMS Detection)\n",
    "\n",
    "**Author:** Your Name  \n",
    "**Date:** 2025-08-26  \n",
    "**Batch:** AUGUST B43\n",
    "\n",
    "This hands-on notebook walks through an end‑to‑end NLP workflow using the classic **SMS Spam Collection** dataset:\n",
    "- Loading data and quick EDA\n",
    "- Text cleaning, tokenization, (optional) stemming\n",
    "- Bag‑of‑Words and **TF‑IDF**\n",
    "- Models: **Naive Bayes**, **Logistic Regression**, **Linear SVM**\n",
    "- Evaluation: Accuracy, Precision, Recall, F1, Confusion Matrix, ROC‑AUC\n",
    "- Save a reusable pipeline and run inference on new messages\n",
    "- *(Optional bonus)* LSTM and BERT stubs for you to extend locally\n",
    "\n",
    "> ⚠️ **Internship requirements** (remember for submission): maintain a `CODSOFT` GitHub repo, keep code original, and share a short LinkedIn demo tagging `@CodSoft` with `#codsoft`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaff967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Setup ===\n",
    "# This notebook uses only standard libraries available in most Python setups.\n",
    "# If something is missing, uncomment and install locally.\n",
    "# !pip install -U scikit-learn pandas numpy matplotlib joblib\n",
    "\n",
    "import os, re, json, math, random, string, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_curve,\n",
    "                             auc, precision_recall_fscore_support, accuracy_score)\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Versions → pandas:\", pd.__version__, \"| numpy:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f5b0c5",
   "metadata": {},
   "source": [
    "## 1. Loading Data 💎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43975a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the dataset. For CodSoft's spam task, typical file name is 'spam.csv'.\n",
    "# If running on Kaggle/Colab, adjust the path accordingly.\n",
    "CSV_PATH = 'spam.csv'  # change if needed\n",
    "\n",
    "# Try reading with common encodings\n",
    "df = None\n",
    "for enc in ['latin-1', 'utf-8', None]:\n",
    "    try:\n",
    "        df = pd.read_csv(CSV_PATH, encoding=enc) if enc else pd.read_csv(CSV_PATH)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Read failed with encoding={enc}: {e}\")\n",
    "\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\"Could not read spam.csv. Make sure the file is in the working directory.\")\n",
    "\n",
    "# The canonical dataset often has these columns: v1 (label), v2 (message), plus unnamed extras.\n",
    "if set(['v1','v2']).issubset(df.columns):\n",
    "    df = df[['v1','v2']].rename(columns={'v1':'label','v2':'text'})\n",
    "else:\n",
    "    # fallback: use first two columns as (label, text)\n",
    "    df = df.iloc[:, :2]\n",
    "    df.columns = ['label','text']\n",
    "\n",
    "# Basic cleanup\n",
    "df = df.dropna(subset=['label','text']).drop_duplicates(subset=['label','text'])\n",
    "df['label'] = df['label'].str.strip().str.lower()\n",
    "df = df[df['label'].isin(['ham','spam'])].copy()\n",
    "\n",
    "# Add message length feature (number of tokens)\n",
    "df['message_len'] = df['text'].apply(lambda s: len(str(s).split()))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6a0702",
   "metadata": {},
   "source": [
    "## 2. EDA 📊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e84b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class distribution\n",
    "counts = df['label'].value_counts().sort_index()\n",
    "print(counts)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.bar(counts.index, counts.values)\n",
    "plt.title('Class Distribution (ham vs spam)')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Message length distribution (separate charts to keep one-plot-per-figure)\n",
    "fig = plt.figure()\n",
    "plt.hist(df[df['label']=='ham']['message_len'], bins=40)\n",
    "plt.title('Message Lengths – HAM')\n",
    "plt.xlabel('Tokens per message')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.hist(df[df['label']=='spam']['message_len'], bins=40)\n",
    "plt.title('Message Lengths – SPAM')\n",
    "plt.xlabel('Tokens per message')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean length | ham:\", df[df.label=='ham']['message_len'].mean(),\n",
    "      \"| spam:\", df[df.label=='spam']['message_len'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e418feb",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing ⚙️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We avoid external downloads by using scikit-learn's ENGLISH_STOP_WORDS.\n",
    "CUSTOM_STOP = set(list(ENGLISH_STOP_WORDS) + ['u','im','c'])\n",
    "\n",
    "URL_RE   = re.compile(r'(https?://\\S+|www\\.\\S+)')\n",
    "EMAIL_RE = re.compile(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b')\n",
    "NUM_RE   = re.compile(r'\\b\\d+\\b')\n",
    "NONALNUM = re.compile(r'[^a-z0-9\\s]')\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Lowercase, replace URLs/emails/numbers, strip punctuation and extra spaces.\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = URL_RE.sub(' URL ', text)\n",
    "    text = EMAIL_RE.sub(' EMAIL ', text)\n",
    "    text = NUM_RE.sub(' NUM ', text)\n",
    "    text = NONALNUM.sub(' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Optional stemming (no NLTK download needed if package is present; otherwise skipped)\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    STEMMER = SnowballStemmer('english')\n",
    "except Exception:\n",
    "    STEMMER = None\n",
    "    print('NLTK not available → skipping stemming.')\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    return ' '.join(tok for tok in text.split() if tok not in CUSTOM_STOP)\n",
    "\n",
    "def stem_text(text: str) -> str:\n",
    "    if STEMMER is None:\n",
    "        return text\n",
    "    return ' '.join(STEMMER.stem(tok) for tok in text.split())\n",
    "\n",
    "def preprocess_pipeline(text: str) -> str:\n",
    "    text = clean_text(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stem_text(text)  # harmless if STEMMER is None\n",
    "    return text\n",
    "\n",
    "# Create a preview column\n",
    "df['text_clean'] = df['text'].apply(preprocess_pipeline)\n",
    "df[['label','text','text_clean']].head(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84d6a2",
   "metadata": {},
   "source": [
    "## 4. Tokens visualization 📊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38223e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CountVectorizer on cleaned text for quick top-token stats\n",
    "cv = CountVectorizer(min_df=2)\n",
    "X_counts = cv.fit_transform(df['text_clean'])\n",
    "vocab = np.array(cv.get_feature_names_out())\n",
    "\n",
    "# Top tokens overall\n",
    "total_counts = np.asarray(X_counts.sum(axis=0)).ravel()\n",
    "top_idx = np.argsort(total_counts)[-20:][::-1]\n",
    "top_tokens = list(zip(vocab[top_idx], total_counts[top_idx]))\n",
    "print(\"Top 20 tokens overall:\") \n",
    "for t,c in top_tokens: \n",
    "    print(f\"{t:>15s} : {c}\")\n",
    "\n",
    "# Plot frequencies\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.bar(range(len(top_tokens)), [c for _,c in top_tokens])\n",
    "plt.xticks(range(len(top_tokens)), [t for t,_ in top_tokens], rotation=45, ha='right')\n",
    "plt.title('Top 20 Tokens Overall')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52870dc1",
   "metadata": {},
   "source": [
    "## 5. Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee85552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "len(X_train), len(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64903c",
   "metadata": {},
   "source": [
    "## 6. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc11b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build three comparable pipelines using TF‑IDF + classic ML\n",
    "pipelines = {\n",
    "    'NaiveBayes': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(preprocessor=preprocess_pipeline, stop_words='english', ngram_range=(1,2), min_df=2)),\n",
    "        ('clf', MultinomialNB(alpha=0.5))\n",
    "    ]),\n",
    "    'LogisticRegression': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(preprocessor=preprocess_pipeline, stop_words='english', ngram_range=(1,2), min_df=2)),\n",
    "        ('clf', LogisticRegression(max_iter=2000, solver='liblinear'))\n",
    "    ]),\n",
    "    'LinearSVM': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(preprocessor=preprocess_pipeline, stop_words='english', ngram_range=(1,2), min_df=2)),\n",
    "        ('clf', CalibratedClassifierCV(LinearSVC(), cv=5))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "metrics_rows = []\n",
    "reports = {}\n",
    "cms = {}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, pos_label='spam', average='binary', zero_division=0)\n",
    "    metrics_rows.append({'model': name, 'accuracy': round(acc,4), 'precision_spam': round(p,4),\n",
    "                         'recall_spam': round(r,4), 'f1_spam': round(f1,4)})\n",
    "    reports[name] = classification_report(y_test, y_pred, digits=4, zero_division=0)\n",
    "    cms[name] = confusion_matrix(y_test, y_pred, labels=['ham','spam'])\n",
    "\n",
    "results = pd.DataFrame(metrics_rows).sort_values('f1_spam', ascending=False).reset_index(drop=True)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f070ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model_name = results.iloc[0]['model']\n",
    "best_pipe = pipelines[best_model_name]\n",
    "\n",
    "print(\"Best model:\", best_model_name)\n",
    "print(\"\\nClassification report:\\n\", reports[best_model_name])\n",
    "\n",
    "# Confusion matrix\n",
    "cm = cms[best_model_name]\n",
    "fig = plt.figure()\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(f'Confusion Matrix – {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.xticks([0,1], ['ham','spam'])\n",
    "plt.yticks([0,1], ['ham','spam'])\n",
    "for (i, j), z in np.ndenumerate(cm):\n",
    "    plt.text(j, i, f\"{z}\", ha='center', va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC curve using Logistic Regression (for smooth probability estimates)\n",
    "lr_pipe = pipelines['LogisticRegression']\n",
    "y_proba_lr = lr_pipe.predict_proba(X_test)[:, list(lr_pipe.classes_).index('spam')]\n",
    "fpr, tpr, thr = roc_curve((y_test=='spam').astype(int), y_proba_lr)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.4f}')\n",
    "plt.plot([0,1], [0,1], linestyle='--')\n",
    "plt.title('ROC Curve – Logistic Regression (spam vs. ham)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239a09e",
   "metadata": {},
   "source": [
    "## 7. Save the best pipeline & Predict on new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b095ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save\n",
    "MODEL_PATH = f\"spam_sms_{best_model_name.lower()}_tfidf_pipeline.joblib\"\n",
    "joblib.dump(best_pipe, MODEL_PATH)\n",
    "print(\"Saved:\", MODEL_PATH)\n",
    "\n",
    "# Quick demo\n",
    "demo_texts = [\n",
    "    \"Congratulations! You have won a $1000 gift card. Click http://bit.ly/xyz to claim now!\", \n",
    "    \"Are we still on for dinner tonight at 7?\", \n",
    "    \"URGENT: Your account was locked due to suspicious activity. Verify at www.example.com\", \n",
    "    \"hey, call me when you're free\"\n",
    "]\n",
    "preds = best_pipe.predict(demo_texts)\n",
    "for t, p in zip(demo_texts, preds):\n",
    "    print(f\"[{p.upper()}] {t}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7c7e7",
   "metadata": {},
   "source": [
    "## 8. (Optional) LSTM — extend locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RUN_DEEP = False  # set True locally if you want to train a quick LSTM\n",
    "\n",
    "if RUN_DEEP:\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "        from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalMaxPool1D\n",
    "\n",
    "        # Tokenize cleaned text\n",
    "        texts = df['text_clean'].tolist()\n",
    "        y_bin = (df['label']=='spam').astype(int).values\n",
    "\n",
    "        tok = Tokenizer(num_words=10000, oov_token='[UNK]')\n",
    "        tok.fit_on_texts(texts)\n",
    "        seqs = tok.texts_to_sequences(texts)\n",
    "        maxlen = max(len(s) for s in seqs)\n",
    "        X_pad = pad_sequences(seqs, maxlen=min(maxlen, 120), padding='post', truncating='post')\n",
    "\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X_pad, y_bin, test_size=0.2, random_state=RANDOM_STATE, stratify=y_bin)\n",
    "\n",
    "        model = Sequential([\n",
    "            Embedding(input_dim=min(10000, len(tok.word_index)+1), output_dim=64, input_length=X_tr.shape[1]),\n",
    "            LSTM(64, return_sequences=True),\n",
    "            GlobalMaxPool1D(),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        history = model.fit(X_tr, y_tr, validation_data=(X_te, y_te), epochs=2, batch_size=32)\n",
    "    except Exception as e:\n",
    "        print(\"Skipping LSTM due to error:\", e)\n",
    "else:\n",
    "    print(\"LSTM section is fully coded but skipped (RUN_DEEP=False). Enable it locally to train.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c75356",
   "metadata": {},
   "source": [
    "## 9. (Optional) BERT — extend locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed22dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is a light stub you can enable locally if you have 'transformers' installed.\n",
    "RUN_BERT = False  # set True locally to try a tiny BERT pipeline\n",
    "\n",
    "if RUN_BERT:\n",
    "    try:\n",
    "        from transformers import BertTokenizerFast, TFBertForSequenceClassification\n",
    "        import tensorflow as tf\n",
    "\n",
    "        texts = df['text_clean'].tolist()\n",
    "        y_bin = (df['label']=='spam').astype(int).values\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(texts, y_bin, test_size=0.2, random_state=RANDOM_STATE, stratify=y_bin)\n",
    "\n",
    "        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        tr_enc = tokenizer(X_tr, truncation=True, padding=True, max_length=128, return_tensors='tf')\n",
    "        te_enc = tokenizer(X_te, truncation=True, padding=True, max_length=128, return_tensors='tf')\n",
    "\n",
    "        model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "        history = model.fit(tr_enc.data, y_tr, validation_data=(te_enc.data, y_te), epochs=1, batch_size=8)\n",
    "    except Exception as e:\n",
    "        print(\"Skipping BERT due to error:\", e)\n",
    "else:\n",
    "    print(\"BERT section is fully coded but skipped (RUN_BERT=False). Enable it locally to train.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528fda2",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Wrap‑up\n",
    "\n",
    "- You now have an end‑to‑end, **original** NLP pipeline for SMS spam detection.\n",
    "- For submission:\n",
    "  1. Push this notebook and the saved `.joblib` model to your **`CODSOFT`** GitHub repo.\n",
    "  2. Record a short demo video and post on LinkedIn tagging **@CodSoft** with **#codsoft**.\n",
    "  3. Keep your code clean, documented, and **do not copy** code verbatim from others.\n",
    "\n",
    "> Tip: For production, wrap the saved pipeline in a small REST/Streamlit app to allow interactive predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e474de",
   "metadata": {},
   "source": [
    "## 11. (Optional) Streamlit Demo 🎛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27873d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A minimal Streamlit demo app for Spam SMS Detection\n",
    "# Save this snippet as app.py and run with: streamlit run app.py\n",
    "\n",
    "import streamlit as st\n",
    "import joblib\n",
    "\n",
    "st.title(\"📱 Spam SMS Detector\")\n",
    "st.write(\"Type an SMS below to check if it's Spam or Ham.\")\n",
    "\n",
    "# Load pipeline\n",
    "pipe = joblib.load(\"spam_sms_naivebayes_tfidf_pipeline.joblib\")\n",
    "\n",
    "user_input = st.text_area(\"Enter SMS text:\", \"\")\n",
    "\n",
    "if st.button(\"Predict\"):\n",
    "    if user_input.strip():\n",
    "        pred = pipe.predict([user_input])[0]\n",
    "        st.success(f\"Prediction: **{pred.upper()}**\")\n",
    "    else:\n",
    "        st.warning(\"Please enter some text.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9177634",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Interactive App (Streamlit) 🚀\n",
    "\n",
    "Run a small web app for live predictions.\n",
    "\n",
    "**How to run locally (terminal):**\n",
    "```bash\n",
    "pip install streamlit joblib scikit-learn\n",
    "streamlit run streamlit_spam_app.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c6f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This cell writes a Streamlit app to the current directory.\n",
    "# After running this cell locally, launch it with:\n",
    "#    streamlit run streamlit_spam_app.py\n",
    "\n",
    "import os, joblib, textwrap\n",
    "\n",
    "APP_PATH = \"streamlit_spam_app.py\"\n",
    "MODEL_GLOB = None\n",
    "\n",
    "# Try to infer the saved model name from this notebook run (fallback to any .joblib file).\n",
    "for f in os.listdir():\n",
    "    if f.endswith(\".joblib\") and \"spam_sms_\" in f:\n",
    "        MODEL_GLOB = f\n",
    "        break\n",
    "if MODEL_GLOB is None:\n",
    "    # fallback: a generic name (user can change in app UI)\n",
    "    MODEL_GLOB = \"spam_sms_naivebayes_tfidf_pipeline.joblib\"\n",
    "\n",
    "app_code = f\"\"\"\n",
    "import os\n",
    "import streamlit as st\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "st.set_page_config(page_title=\"Spam SMS Detector\", page_icon=\"📱\", layout=\"centered\")\n",
    "\n",
    "st.title(\"📱 Spam SMS Detector\")\n",
    "st.caption(\"TF-IDF + Classic ML (CodSoft Task 4)\")\n",
    "\n",
    "@st.cache_resource\n",
    "def load_pipeline(path: str):\n",
    "    return joblib.load(path)\n",
    "\n",
    "# Sidebar: model selection\n",
    "st.sidebar.header(\"Model\")\n",
    "default_model_path = \"{MODEL_GLOB}\"\n",
    "model_file = st.sidebar.text_input(\"Path to .joblib pipeline\", value=default_model_path)\n",
    "uploaded = st.sidebar.file_uploader(\"...or upload a .joblib file\", type=[\"joblib\"])\n",
    "\n",
    "pipeline = None\n",
    "if uploaded is not None:\n",
    "    tmp_path = \"uploaded_pipeline.joblib\"\n",
    "    with open(tmp_path, \"wb\") as f:\n",
    "        f.write(uploaded.read())\n",
    "    model_file = tmp_path\n",
    "\n",
    "try:\n",
    "    pipeline = load_pipeline(model_file)\n",
    "    st.sidebar.success(f\"Loaded: {model_file}\")\n",
    "except Exception as e:\n",
    "    st.sidebar.warning(\"Could not load pipeline. Use the text box or upload a .joblib file.\")\n",
    "\n",
    "st.subheader(\"Try it out\")\n",
    "sms = st.text_area(\"Paste an SMS message here:\", height=120, placeholder=\"e.g., Congratulations! You won a prize. Claim at http://bit.ly/...\")\n",
    "\n",
    "def predict_one(text: str):\n",
    "    if pipeline is None:\n",
    "        return None, None\n",
    "    pred = pipeline.predict([text])[0]\n",
    "    prob = None\n",
    "    # Try to show probabilities if available\n",
    "    if hasattr(pipeline, \"predict_proba\"):\n",
    "        try:\n",
    "            proba = pipeline.predict_proba([text])[0]\n",
    "            # proba is [P(ham), P(spam)] or similar; find 'spam' index\n",
    "            classes = list(getattr(pipeline, \"classes_\", []))\n",
    "            if classes and \"spam\" in classes:\n",
    "                prob = float(proba[classes.index(\"spam\")])\n",
    "            else:\n",
    "                # fallback to max prob\n",
    "                prob = float(np.max(proba))\n",
    "        except Exception:\n",
    "            prob = None\n",
    "    return pred, prob\n",
    "\n",
    "col1, col2 = st.columns([1,1])\n",
    "with col1:\n",
    "    if st.button(\"Predict\"):\n",
    "        if not sms.strip():\n",
    "            st.info(\"Please paste a message first.\")\n",
    "        else:\n",
    "            label, prob = predict_one(sms)\n",
    "            if label is None:\n",
    "                st.error(\"Pipeline not loaded.\")\n",
    "            else:\n",
    "                st.markdown(f\"**Prediction:** :{'green_circle' if label=='ham' else 'red_circle'}: **{label.upper()}**\")\n",
    "                if prob is not None:\n",
    "                    st.write(f\"Spam probability: **{prob:.3f}**\")\n",
    "with col2:\n",
    "    st.write(\"Examples\")\n",
    "    samples = [\n",
    "        \"Congratulations! You have won a $1000 gift card. Click http://bit.ly/xyz to claim now!\",\n",
    "        \"Are we still on for dinner tonight at 7?\",\n",
    "        \"URGENT: Your account was locked due to suspicious activity. Verify at www.security-check.com\",\n",
    "        \"hey, call me when you're free\",\n",
    "    ]\n",
    "    if st.button(\"Run examples\"):\n",
    "        for s in samples:\n",
    "            label, prob = predict_one(s)\n",
    "            st.write(f\"- **{label.upper() if label else 'N/A'}** — {s}\")\n",
    "            if prob is not None:\n",
    "                st.caption(f\"Spam probability: {prob:.3f}\")\n",
    "\n",
    "st.sidebar.markdown(\"—\")\n",
    "st.sidebar.markdown(\"**Tips**\")\n",
    "st.sidebar.markdown(\"- Use the same preprocessing vectorizer embedded in your saved pipeline.\")\n",
    "st.sidebar.markdown(\"- For consistent results, use the `.joblib` produced by this notebook.\")\n",
    "\"\"\"\n",
    "\n",
    "with open(APP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"Wrote:\", APP_PATH)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
